The topic is how to amplify human ability.
# Supervising Strong Learners by Amplifying Weak Experts

## Summary
When we trained an ML system, we need some training signals to evaluate how well it did. The signals can be automatically generated by algorithms. They can be labels or reward functions designed by humans. However, there are much more taks where humans are not able to specify accurate training signals. The simple but unaccurate proxy will cause misalignment problems.

In this paper, the authors propose a general framework called Iterated Amplification. Suppose we have a human expert H, and an agent X needed to be trained, H will divide the task into a sequence of simpler subtasks. The copies of X will solve these subtasks and H will give the final solution with the help of these copies. The complex system consisting of H and agents can be a teacher to train the agent X.

At first, X can only give useless solutions but H can solve the task independently. X gradually learns how to solve simpler tasks. Then the system will be a better teacher to train X. The main idea is that the system consisting of several copies of the agents can always provide a slightly better soultion than X.

In this paper, the author carried out several experiments on question-answering problems. There are still many problems leaved to be solved. I think there are two main problem. First, in this framework, it seems that the subtask must belongs to the same kind of tasks. In other words, the same agent X must be able solve a task and its subtasks. Some realistic problems may not satisfy this. The second problem is if human experts are not be able to decompose the task, how to use iteration amplification framework? 

So this is may summary about this paper. Do you have anything to add?

# Recursive Reward Modelling
## summary
Reward modelling means training to learn a reward model from feedback. In some cases, it is difficult from humans to provide feedback because they have no way to evaluate the outcomes. Recursive reward modelling help humans to boost their ability to provide feedback.

First, we train agent $A_{1}$ with reward modelling from human feedback. In step k, we use the agent $A_{k-1}$ to assit the user in evaluating the outcomes of agent $A_{k}$.

This method is facing two difficulties. First, how should the user decompose task evaluation. This decomposition need to be exhausitive to make $A_{k-1}$ can perfectly evaluate $A_{k}$. Second, will the errors accumulate? It is needed to prove that the mistake of the more narrow agent will not lead to larer mistakes in the training of more general agent.

I did not understand the last part of this section because I have no knowledge anout first-order logic or NP-complete problems. If you have some ideas about this part, please share to me.

# Iterated Distillation and Amplification
## summary
Sometimes we have to face the difficulty of the alignment/capabilities tradeoff. If we use broad reinforcement learning or broad inverse reinforcement learning, the agent can explore novel actions and become more capable. However, it is hard to align them because a broad objective is hard to design. If we use narrow reinforcement learning or imitation learning, we may not be able to get a superhuman agent.

IDA (Iterated Distillation and Amplification) apply a similar strategy with AlphaGoZero. First, suppose we have trained an agent A[0] with techniques like imitation learning, we can run a number of A[0] and let them do some part of the tasks. Humans can make better decision with the help of these agents. Then a better agent A[1] can be trained with imitation learning.

Now we need evidence to make sure the distill procedure and the amplify procedure are both robustly preserves alignment. We aslo want to prove that at least some human experts are able to iteratively apply amplification to achieve arbitrarily high capabilities at the relevant task. Because the final agent is not more complex than A[0], it is efficient after deployed. However, we still need to consider whether it is feasible with limited resource and time cost to train in this way.

